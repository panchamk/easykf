@article{Nakahara2002,
	abstract = {Self-organization is one of fundamental brain computations for forming efficient representations of information. Experimental support for this idea has been largely limited to the developmental and reorganizational formation of neural circuits in the sensory cortices. We now propose that self-organization may also play an important role in short-term synaptic changes in reward-driven voluntary behaviors. It has recently been shown that many neurons in the basal ganglia change their sensory responses flexibly in relation to rewards. Our computational model proposes that the rapid changes in striatal projection neurons depend on the subtle balance between the Hebb-type mechanisms of excitation and inhibition, which are modulated by reinforcement signals. Simulations based on the model are shown to produce various types of neural activity similar to those found in experiments.},
	affiliation = {Laboratory for Mathematical Neuroscience, RIKEN Brain Science Institute 2-1 Hirosawa, Wako, Saitama, 351-0198, Japan. hiro@brain.riken.go.jp},
	author = {H. Nakahara and Amari S. and O. Hikosaka},
	issn = {0899-7667},
	journal = {Neural Comput},
	number = {4},
	pages = {819--44},
	title = {Self-organization in the basal ganglia with modulation of reinforcement signals.},
	volume = {14},
	year = {2002}
}

@inproceedings{Amari1983,
	author = {S. Amari},
	booktitle = {IEEE Trans. Systems, Man and Cybernetics},
	number = {9--10},
	pages = {741--748},
	title = {Field theory of self-organizing neural nets},
	volume = {SMC-13},
	year = {1983}
}

@phdthesis{Merwe2004,
	author = {R. {van der Merwe}},
	school = {OGI School of Science \& Engeneering at Oregon Health \& Science University, Portland, OR, USA},
	title = {Sigma-Point Kalman Filters for Probabilistic Inference in Dynamic State-Space models},
	year = {2004}
}

@article{Saeb2009,
	abstract = {The brain is able to perform actions based on an adequate internal representation of the world, where task-irrelevant features are ignored and incomplete sensory data are estimated. Traditionally, it is assumed that such abstract state representations are obtained purely from the statistics of sensory input for example by unsupervised learning methods. However, more recent findings suggest an influence of the dopaminergic system, which can be modeled by a reinforcement learning approach. Standard reinforcement learning algorithms act on a single layer network connecting the state space to the action space. Here, we involve in a feature detection stage and a memory layer, which together, construct the state space for a learning agent. The memory layer consists of the state activation at the previous time step as well as the previously chosen action. We present a temporal difference based learning rule for training the weights from these additional inputs to the state layer. As a result, the performance of the network is maintained both, in the presence of task-irrelevant features, and at randomly occurring time steps during which the input is invisible. Interestingly, a goal-directed forward model emerges from the memory weights, which only covers the state-action pairs that are relevant to the task. The model presents a link between reinforcement learning, feature detection and forward models and may help to explain how reward systems recruit cortical circuits for goal-directed feature detection and prediction.},
	affiliation = {Frankfurt Institute for Advanced Studies, Goethe University, Frankfurt am Main, Germany. saeb@fias.uni-frankfurt.de},
	author = {S. Saeb and C. Weber and J. Triesch},
	issn = {1879-2782},
	journal = {Neural Netw},
	number = {5-6},
	pages = {586--92},
	title = {Goal-directed learning of features and forward models.},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/19616917},
	volume = {22},
	year = 2009
}

@inproceedings{Weber2008,
	author = {C. Weber and J. Triesch},
	booktitle = {ICANN 2008, LCNS 5163},
	pages = {740--749},
	publisher = {Springer-Verlag},
	title = {From exploration to planning},
	year = {2008}
}

@article{Rougier2010,
	author = {N.P. Rougier and Y. Boniface},
	journal = {Neurocomputing},
	title = {Dynamic self organizing maps},
	year = {2010}
}

@inproceedings{Alecu2008,
	author = {L. Alecu and H. Frezza-Buet},
	booktitle = {Proceedings of the second french conference on Computational Neuroscience, Neurocomp},
	title = {An empirical evaluation framework for qualifying dynamic neural fields},
	year = {2008}
}

@inproceedings{Alecu2009,
	author = {L. Alecu and H. Frezza-Buet},
	booktitle = {Neural Information Processing, ICONIP'09 Proceedings, Part I,},
	pages = {135--142},
	title = {Application-driven parameter tuning methodology for dynamic neural field equations},
	year = {2009}
}

@inproceedings{Quinton2010,
	author = {J.C. Quinton},
	booktitle = {Proceedings of IEEE World Congress on Computational Intelligence, IJCNN 2010},
	title = {Exploring and Optimizing Dynamic Neural Fields Parameters Using Genetic Algorithms},
	year = {2010}
}

@article{Igel2001,
	author = {C. Igel and W. Erlhagen and D. Jancke},
	journal = {Neurocomputing},
	number = {1--4},
	title = {Optimization of dynamic neural fields},
	volume = {36},
	year = {2001}
}

@inproceedings{Gross1996,
	author = {H. Gross and V. Stephan and H. B{\"o}hme},
	booktitle = {Proc WCNN 1996},
	title = {Sensory-based robot navigation using self-organizing networks and Q-Learning},
	year = {1996}
}

@inproceedings{Gross1998,
	author = {H. Gross and V. Stephan and M. Krabbes},
	booktitle = {IEEE WCCI-IJCNN 1998},
	title = {A neural field approach to topological reinforcement learning in continuous action spaces},
	year = {1998}
}

@inproceedings{Weber2009,
	author = {C. Weber and J. Triesch},
	booktitle = {Proc. IJCNN 2009},
	title = {Goal-directed feature learning},
	year = {2009}
}

@book{Obermayer2001,
	author = {K. Obermayer and T. Sejnowski},
	publisher = {MIT Press},
	title = {Self-organizing Map Formation : Foundations of Neural Computation},
	year = {2001}
}

@article{Zomaya2001,
	author = {A.Y Zomaya and T. Yee-Hwei},
	journal = {Parallel and Distributed Systems, IEEE Transactions},
	number = {9},
	title = {Observations on Using Genetic Algorithms for Dynamic Load-Balancing},
	volume = {12},
	year = {2001}
}

@article{Friston2010,
	abstract = {A free-energy principle has been proposed recently that accounts for action, perception and learning. This Review looks at some key brain theories in the biological (for example, neural Darwinism) and physical (for example, information theory and optimal control theory) sciences from the free-energy perspective. Crucially, one key theme runs through each of these theories - optimization. Furthermore, if we look closely at what is optimized, the same quantity keeps emerging, namely value (expected reward, expected utility) or its complement, surprise (prediction error, expected cost). This is the quantity that is optimized under the free-energy principle, which suggests that several global brain theories might be unified within a free-energy framework.},
	affiliation = {The Wellcome Trust Centre for Neuroimaging, University College London, Queen Square, London, WC1N 3BG, UK. k.friston@fil.ion.ucl.ac.uk},
	author = {Karl Friston},
	issn = {1471-0048},
	journal = {Nat Rev Neurosci},
	number = {2},
	pages = {127--38},
	title = {The free-energy principle: a unified brain theory?},
	volume = {11},
	year = {2010}
}

@article{Pfeiffer2010,
	abstract = {We introduce a framework for decision making in which the learning of decision making is reduced to its simplest and biologically most plausible form: Hebbian learning on a linear neuron. We cast our Bayesian-Hebb learning rule as reinforcement learning in which certain decisions are rewarded and prove that each synaptic weight will on average converge exponentially fast to the log-odd of receiving a reward when its pre- and postsynaptic neurons are active. In our simple architecture, a particular action is selected from the set of candidate actions by a winner-take-all operation. The global reward assigned to this action then modulates the update of each synapse. Apart from this global reward signal, our reward-modulated Bayesian Hebb rule is a pure Hebb update that depends only on the coactivation of the pre- and postsynaptic neurons, not on the weighted sum of all presynaptic inputs to the postsynaptic neuron as in the perceptron learning rule or the Rescorla-Wagner rule. This simple approach to action-selection learning requires that information about sensory inputs be presented to the Bayesian decision stage in a suitably preprocessed form resulting from other adaptive processes (acting on a larger timescale) that detect salient dependencies among input features. Hence our proposed framework for fast learning of decisions also provides interesting new hypotheses regarding neural nodes and computational goals of cortical areas that provide input to the final decision stage.},
	affiliation = {Institute for Theoretical Computer Science, Graz University of Technology, A-8010 Graz, Austria. pfeiffer@igi.tugraz.at},
	author = {M. Pfeiffer and B. Nessler and R. Douglas and W. Maass},
	issn = {1530-888X},
	journal = {Neural Comput},
	number = {6},
	pages = {1399--444},
	title = {Reward-modulated Hebbian learning of decision making.},
	volume = {22},
	year = {2010}
}

@article{Steimer2009,
	abstract = {From a theoretical point of view, statistical inference is an attractive model of brain operation. However, it is unclear how to implement these inferential processes in neuronal networks. We offer a solution to this problem by showing in detailed simulations how the belief propagation algorithm on a factor graph can be embedded in a network of spiking neurons. We use pools of spiking neurons as the function nodes of the factor graph. Each pool gathers "messages" in the form of population activities from its input nodes and combines them through its network dynamics. Each of the various output messages to be transmitted over the edges of the graph is computed by a group of readout neurons that feed in their respective destination pools. We use this approach to implement two examples of factor graphs. The first example, drawn from coding theory, models the transmission of signals through an unreliable channel and demonstrates the principles and generality of our network approach. The second, more applied example is of a psychophysical mechanism in which visual cues are used to resolve hypotheses about the interpretation of an object's shape and illumination. These two examples, and also a statistical analysis, demonstrate good agreement between the performance of our networks and the direct numerical evaluation of belief propagation.},
	affiliation = {Institute of Neuroinformatics, University of Z{\~A}¼rich, and ETH Z{\~A}¼rich, Z{\~A}¼rich, 8057 Switzerland. asteimer@ini.phys.ethz.ch},
	author = {Andreas Steimer and Wolfgang Maass and Rodney Douglas},
	issn = {0899-7667},
	journal = {Neural Comput},
	number = {9},
	pages = {2502--23},
	title = {Belief propagation in networks of spiking neurons.},
	volume = {21},
	year = {2009}
}

@article{Bruce2009,
	abstract = {A proposal for saliency computation within the visual cortex is put forth based on the premise that localized saliency computation serves to maximize information sampled from one's environment. The model is built entirely on computational constraints but nevertheless results in an architecture with cells and connectivity reminiscent of that appearing in the visual cortex. It is demonstrated that a variety of visual search behaviors appear as emergent properties of the model and therefore basic principles of coding and information transmission. Experimental results demonstrate greater efficacy in predicting fixation patterns across two different data sets as compared with competing models.},
	affiliation = {Department of Computer Science \& Engineering and Centre for Vision Research, York University, Toronto, ON, Canada. neil@cse.yorku.ca},
	author = {N. Bruce and J. Tsotsos},
	issn = {1534-7362},
	journal = {J Vis},
	number = {3},
	pages = {5.1--24},
	title = {Saliency, attention, and visual search: an information theoretic approach.},
	volume = {9},
	year = {2009}
}

@article{Wolpert2007,
	abstract = {Sensory and motor uncertainty form a fundamental constraint on human sensorimotor control. Bayesian decision theory (BDT) has emerged as a unifying framework to understand how the central nervous system performs optimal estimation and control in the face of such uncertainty. BDT has two components: Bayesian statistics and decision theory. Here we review Bayesian statistics and show how it applies to estimating the state of the world and our own body. Recent results suggest that when learning novel tasks we are able to learn the statistical properties of both the world and our own sensory apparatus so as to perform estimation using Bayesian statistics. We review studies which suggest that humans can combine multiple sources of information to form maximum likelihood estimates, can incorporate prior beliefs about possible states of the world so as to generate maximum a posteriori estimates and can use Kalman filter-based processes to estimate time-varying states. Finally, we review Bayesian decision theory in motor control and how the central nervous system processes errors to determine loss functions and select optimal actions. We review results that suggest we plan movements based on statistics of our actions that result from signal-dependent noise on our motor outputs. Taken together these studies provide a statistical framework for how the motor system performs in the presence of uncertainty.},
	affiliation = {Computational and Biological Learning Group, Department of Engineering, University of Cambridge, Trumpington Street, Cambridge CB2 1PZ, Cambridge, UK. wolpert@eng.cam.ac.uk},
	author = {D. Wolpert},
	issn = {0167-9457},
	journal = {Hum Mov Sci},
	number = {4},
	pages = {511--24},
	title = {Probabilistic models in human sensorimotor control.},
	volume = {26},
	year = {2007}
}

@article{Beck2008,
	abstract = {When making a decision, one must first accumulate evidence, often over time, and then select the appropriate action. Here, we present a neural model of decision making that can perform both evidence accumulation and action selection optimally. More specifically, we show that, given a Poisson-like distribution of spike counts, biological neural networks can accumulate evidence without loss of information through linear integration of neural activity and can select the most likely action through attractor dynamics. This holds for arbitrary correlations, any tuning curves, continuous and discrete variables, and sensory evidence whose reliability varies over time. Our model predicts that the neurons in the lateral intraparietal cortex involved in evidence accumulation encode, on every trial, a probability distribution which predicts the animal's performance. We present experimental evidence consistent with this prediction and discuss other predictions applicable to more general settings.},
	affiliation = {Department of Brain and Cognitive Sciences, University of Rochester, Rochester, NY 14627, USA.},
	author = {Jeffrey M Beck and Wei Ji Ma and Roozbeh Kiani and Tim Hanks and Anne K Churchland and Jamie Roitman and Michael N Shadlen and Peter E Latham and Alexandre Pouget},
	issn = {1097-4199},
	journal = {Neuron},
	number = {6},
	pages = {1142--52},
	title = {Probabilistic population codes for Bayesian decision making.},
	volume = {60},
	year = {2008}
}

@article{Deneve2007,
	abstract = {Several behavioral experiments suggest that the nervous system uses an internal model of the dynamics of the body to implement a close approximation to a Kalman filter. This filter can be used to perform a variety of tasks nearly optimally, such as predicting the sensory consequence of motor action, integrating sensory and body posture signals, and computing motor commands. We propose that the neural implementation of this Kalman filter involves recurrent basis function networks with attractor dynamics, a kind of architecture that can be readily mapped onto cortical circuits. In such networks, the tuning curves to variables such as arm velocity are remarkably noninvariant in the sense that the amplitude and width of the tuning curves of a given neuron can vary greatly depending on other variables such as the position of the arm or the reliability of the sensory feedback. This property could explain some puzzling properties of tuning curves in the motor and premotor cortex, and it leads to several new predictions.},
	affiliation = {Group for Neural Theory, D{\~A}©partement d'Etude Cognitives, Ecole Normale Sup{\~A}©rieure, Coll{\~A}¨ge de France, Centre National de la Recherche Scientifique, 75005 Paris, France. sophie.deneve@ens.fr},
	author = {S. Den{\`e}ve and J. Duhamel and A. Pouget},
	journal = {J Neurosci},
	number = {21},
	pages = {5744--56},
	title = {Optimal sensorimotor integration in recurrent cortical networks: a neural implementation of Kalman filters.},
	volume = {27},
	year = {2007}
}

@techreport{Welch2006,
	author = {G. Welch and G. Bishop},
	institution = {Department of Computer science - University of North California},
	number = {Technical Report TR95-041},
	title = {An introduction to the Kalman filter},
	year = {2006}
}

@book{Gelb1986,
	author = {A. Gelb},
	publisher = {MIT Press},
	title = {Applied Optimal Estimation},
	year = {1986}
}

@inproceedings{Julier2004,
	abstract = {The extended Kalman filter (EKF) is probably the most widely used estimation algorithm for nonlinear systems. However, more than 35 years of experience in the estimation community has shown that is difficult to implement, difficult to tune, and only reliable for systems that are almost linear on the time scale of the updates. Many of these difficulties arise from its use of linearization. To overcome this limitation, the unscented transformation (UT) was developed as a method to propagate mean and covariance information through nonlinear transformations. It is more accurate, easier to implement, and uses the same order of calculations as linearization. This paper reviews the motivation, development, use, and implications of the UT. Keywords---Estimation, Kalman filtering, nonlinear systems, target tracking. I.},
	author = {S.J. Julier and J.K. Uhlmann},
	booktitle = {Proc IEEE},
	pages = {401--422},
	title = {Unscented Filtering and Nonlinear Estimation},
	year = {2004}
}

@inproceedings{Julier1997,
	author = {S.J. Julier and J.K. Uhlmann},
	booktitle = {Int. Symp. Aerospace/Defense Sensing, Simul. and Controls},
	organization = {Citeseer},
	pages = {21--24},
	title = {A new extension of the Kalman filter to nonlinear systems},
	volume = {3},
	year = {1997}
}

